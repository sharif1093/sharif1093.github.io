[{"authors":null,"categories":null,"content":"Mohammadreza Sharif is a Ph.D. candidate in RIVeR lab, Northeastern University. His research focuses on developing human-robot interaction algorithms. He joined Northeastern University back in 2015. Being an enthusiastic programmer, he has developed several packages and frameworks for deep reinforcement learning, deep learning, and robotics.\n","date":1618707603,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1618707603,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sharif1093.github.io/author/mohammadreza-sharif/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mohammadreza-sharif/","section":"authors","summary":"Mohammadreza Sharif is a Ph.D. candidate in RIVeR lab, Northeastern University. His research focuses on developing human-robot interaction algorithms. He joined Northeastern University back in 2015. Being an enthusiastic programmer, he has developed several packages and frameworks for deep reinforcement learning, deep learning, and robotics.","tags":null,"title":"Mohammadreza Sharif","type":"authors"},{"authors":["Mehrshad Zandigohar","Mo Han","Mohammadreza Sharif","Sezen Yagmur Gunay","Mariusz P. Furmanek","Mathew Yarossi","Paolo Bonato","Cagdas Onal","Taskin Padir","Deniz Erdogmus","Gunar Schirner"],"categories":[],"content":"","date":1618707603,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618707603,"objectID":"42cf321f3570d54ae548959053c2027a","permalink":"https://sharif1093.github.io/publication/2021-preprint-a/","publishdate":"2021-04-17T21:00:03-04:00","relpermalink":"/publication/2021-preprint-a/","section":"publication","summary":"For lower arm amputees, robotic prosthetic hands offer the promise to regain the capability to perform fine object manipulation in activities of daily living. Accurate inference of the human's intended gesture to control a robotic prosthetic hand is vital to the efficacy of the solution. Current control methods based on physiological signals such as electroencephalography (EEG) and electromyography (EMG) are prone to yielding poor inference outcomes due to motion artifacts, variability of skin electrode junction impedance over time, muscle fatigue, and other factors. Vision sensors are a major source of information about the environment state and can play a vital role in inferring feasible and intended gestures. However, visual evidence is also susceptible to its own artifacts, most often due to object occlusion, lighting changes, variable shapes of objects depending on view-angle, among other factors. Multimodal evidence fusion using physiological and vision sensor measurements is a natural approach due to the complementary strengths of these modalities.\nIn this paper, we present a Bayesian evidence fusion framework for grasp intent inference using eye-view video, eye-gaze, and EMG from the forearm processed by neural network models. We analyze individual and fused performance as a function of time as the hand approaches the object to grasp it. For this purpose, we have also developed novel data processing and augmentation techniques to train neural network components. Our experimental data analyses demonstrate that= EMG and visual evidence show complementary strengths, and as a consequence, fusion of multimodal evidence can outperform each individual evidence modality at any given time. Specifically, results indicate that, on average, fusion improves the instantaneous upcoming grasp type classification accuracy while in the reaching phase by 13.66% and 14.8%, relative to EMG and visual evidence individually. An overall fusion accuracy of 95.3% among 13 labels (compared to a chance level of 7.7%) is achieved, and more detailed analysis indicate that the correct grasp is inferred sufficiently early and with high confidence compared to the top contender, in order to allow successful robot actuation to close the loop. ","tags":["Sensor Fusion","Multimodal","Human-in-the-Loop","Object Detection","EMG Classification"],"title":"Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control","type":"publication"},{"authors":["Mohammadreza Sharif","Deniz Erdogmus","Christopher Amato","Taskin Padir"],"categories":[],"content":"","date":1618700343,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618700343,"objectID":"ea8b8ff5b9c321ec5f51e43d441ad5ff","permalink":"https://sharif1093.github.io/publication/2021-icra/","publishdate":"2021-04-17T18:59:03-04:00","relpermalink":"/publication/2021-icra/","section":"publication","summary":"State-of-the-art human-in-the-loop robot grasping is hugely suffered by Electromyography (EMG) inference robustness issues. As a workaround, researchers have been looking into integrating EMG with other signals, often in an ad hoc manner. In this paper, we are presenting a method for end-to-end training of a policy for human-in-the-loop robot grasping on real reaching trajectories. For this purpose we use Reinforcement Learning (RL) and Imitation Learning (IL) in DEXTRON (DEXTerity enviRONment), a stochastic simulation environment with real human trajectories that are augmented and selected using a Monte Carlo (MC) simulation method. We also offer a success model which once trained on the expert policy data and the RL policy roll-out transitions, can provide transparency to how the deep policy works and when it is probably going to fail.","tags":["Reinforcement Learning","Human-in-the-Loop","Human Robot Collaboration"],"title":"End-to-end grasping policies for human-in-the-loop robots via deep reinforcement learning","type":"publication"},{"authors":["Mohammadreza Sharif","Christopher Amato","Deniz Erdogmus","Taskin Padir"],"categories":[],"content":"","date":1604444343,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604444343,"objectID":"69a981b25220b33bddca00f58ff68113","permalink":"https://sharif1093.github.io/publication/2020-biorob/","publishdate":"2020-10-15T18:59:03-04:00","relpermalink":"/publication/2020-biorob/","section":"publication","summary":"Robot prosthetic hands intend to replicate oneâ€™s lost abilities through intuitive control. So far, control methods that rely heavily on the human input such as Electromyographic (EMG) and Electroneurographic (ENG) signals have been predominantly studied. However, these methods face issues such as lack of robustness resulting in abandonment of this technology by the users. There is a need for a paradigm shift in the robot prosthetic hand control methods. With this regard, we propose an end-to-end learning of control policy for a robot prosthetic hand through reinforcement learning. Imitation learning has been fostered to help with the sparse reward setting in the hard-to-explore state-space of the problem. The results in simulation show the feasibility of successfully learning an endto-end policy for grasping objects by robot prosthetic hands, potentially increasing robustness for grasp control of future robot prosthetic hands.","tags":["Reinforcement Learning","Human-in-the-Loop","Human Robot Collaboration"],"title":"Towards End-to-End Control of a Robot Prosthetic Hand via Reinforcement Learning","type":"publication"},{"authors":["Mohammadreza Sharif","Deniz Erdogmus","Taskin Padir"],"categories":[],"content":"","date":1563664547,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563664547,"objectID":"2b18e7ff4f46ba7c8ebd1b6edb8b5dd2","permalink":"https://sharif1093.github.io/publication/2019-ijrc/","publishdate":"2020-05-19T19:15:47-04:00","relpermalink":"/publication/2019-ijrc/","section":"publication","summary":"Robotic prosthetic hands are commonly controlled using electromyography (EMG) signals as a means of inferring user intention. However, relying on EMG signals alone, although provides very good results in lab settings, is not sufficiently robust to real-life conditions. For this reason, taking advantage of other contextual clues are proposed in previous works. In this work, we propose a method for intention inference based on particle filtering (PF) based on user hand's trajectory information. Our methodology, also provides an estimate of time-to-arrive, i.e. time left until reaching to the object, which is an essential variable in successful grasping of objects. The proposed probabilistic framework can incorporate available sources of information to improve the inference process. We also provide a data-driven method based on hidden Markov model (HMM) as a baseline for intention inference. HMM is widely used for human gesture classification. The algorithms were tested (and trained) with regards to 160 reaching trajectories collected from 10 subjects reaching to one of four objects at a time. The results show a classification accuracy of 91.3% and 82.2% for the entire reaching period for the PF and HMM methods, respectively.","tags":[],"title":"Particle Filters vs Hidden Markov Models for Prosthetic Robot Hand Grasp Selection","type":"publication"},{"authors":["Mohammadreza Sharif","Deniz Erdogmus","Taskin Padir"],"categories":[],"content":"","date":1551135543,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551135543,"objectID":"02313c7fe4979543ebabffbe63604614","permalink":"https://sharif1093.github.io/publication/2019-irc/","publishdate":"2020-05-19T18:59:03-04:00","relpermalink":"/publication/2019-irc/","section":"publication","summary":"Human intent inference is a key factor in shared autonomy. The control of robotic prosthetic hands with inherent human-in-the-loop operations is not an exception. State-of-the-art methods use electromyography (EMG) signals to infer grasp types. But this approach is limited by important robustness issues which renders its use limited outside labs. In this paper, we use a particle filtering based technique to infer user intent based on the trajectories of the user's hand. Our methodology generates an estimation of the expected time-to-arrive, i.e. time left until reaching to the object, which is an essential variable in successful grasping of objects. The approach is integrated using a probabilistic framework which can incorporate information from all available sources. We have tested our algorithm with 80 reaching trajectories collected from 5 individuals reaching to one of four objects at a time. The results demonstrate a classification accuracy of 87.5% for the entire reaching period.","tags":[],"title":"Human-in-the-Loop Prosthetic Robot Hand Control Using Particle Filters for Grasp Selection","type":"publication"},{"authors":["Yin Wang","Bengisu Ozbay","Mohammadreza Sharif","Mario Sznaier"],"categories":[],"content":"","date":1481581153,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481581153,"objectID":"bb4dba1168d8e976c09f06f03471c4bf","permalink":"https://sharif1093.github.io/publication/2016-cdc/","publishdate":"2020-05-19T18:19:13-04:00","relpermalink":"/publication/2016-cdc/","section":"publication","summary":"In this paper we consider the synthesis of sparse stabilizing static state feedback controllers subject to state and control constraints. The presence of information constraints renders this problem generically NP-hard. However, as we show in the paper, a convergent sequence of tractable convex relaxations with optimality certificates can be obtained by transforming the problem into a polynomial optimization form. The effectiveness of the proposed technique is illustrated with a numerical example.","tags":[],"title":"A convex optimization approach to design of information structured linear constrained controllers","type":"publication"}]